2024-12-15 02:40:13,042 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 0, 'avg_loss': 2.1594300626924166, 'accuracy': 0.2392857142857143, 'precision': 0.37006802721088433, 'recall': 0.2392857142857143, 'f1': 0.2700144061542611, 'macro_accuracy': 0.2392857142857143, 'macro_precision': 0.21904761904761902, 'macro_recall': 0.2220843570843571, 'macro_f1': 0.19557854521317292, 'question_accuracy': 0.22857142857142856, 'mean_confidence': 0.99993646}
2024-12-15 02:43:16,804 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 1, 'avg_loss': 1.7755349829400842, 'accuracy': 0.2642857142857143, 'precision': 0.3581147261635188, 'recall': 0.2642857142857143, 'f1': 0.29156867245102536, 'macro_accuracy': 0.2642857142857143, 'macro_precision': 0.21170371839583918, 'macro_recall': 0.2156224406224406, 'macro_f1': 0.20166996490525904, 'question_accuracy': 0.22857142857142856, 'mean_confidence': 0.9999978}
2024-12-15 02:46:21,263 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 2, 'avg_loss': 1.9425473241625648, 'accuracy': 0.19285714285714287, 'precision': 0.3458460191626584, 'recall': 0.19285714285714287, 'f1': 0.23276927148953983, 'macro_accuracy': 0.19285714285714287, 'macro_precision': 0.17083479496830664, 'macro_recall': 0.14059377559377556, 'macro_f1': 0.13733410909693913, 'question_accuracy': 0.14285714285714285, 'mean_confidence': 0.93948865}
2024-12-15 02:49:26,416 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 3, 'avg_loss': 2.844404164039971, 'accuracy': 0.20714285714285716, 'precision': 0.3896096997151263, 'recall': 0.20714285714285716, 'f1': 0.2342183408326105, 'macro_accuracy': 0.20714285714285716, 'macro_precision': 0.2259684537430533, 'macro_recall': 0.21571662571662573, 'macro_f1': 0.18640149238291526, 'question_accuracy': 0.17142857142857143, 'mean_confidence': 0.95596325}
2024-12-15 02:52:30,960 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 4, 'avg_loss': 2.5456743258002557, 'accuracy': 0.18928571428571428, 'precision': 0.33679222255576935, 'recall': 0.18928571428571428, 'f1': 0.21313927909898528, 'macro_accuracy': 0.18928571428571428, 'macro_precision': 0.19413448620345175, 'macro_recall': 0.19541359541359543, 'macro_f1': 0.16634882587652483, 'question_accuracy': 0.15714285714285714, 'mean_confidence': 0.99392796}
2024-12-15 02:55:36,313 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 5, 'avg_loss': 2.5861893479075087, 'accuracy': 0.15, 'precision': 0.27824220834475777, 'recall': 0.15, 'f1': 0.17549979258297035, 'macro_accuracy': 0.15, 'macro_precision': 0.15223469894456934, 'macro_recall': 0.14272317772317772, 'macro_f1': 0.12523563293088485, 'question_accuracy': 0.14285714285714285, 'mean_confidence': 0.97630954}
2024-12-15 02:58:40,660 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 6, 'avg_loss': 2.719055968581846, 'accuracy': 0.5178571428571429, 'precision': 0.32166405023547884, 'recall': 0.5178571428571429, 'f1': 0.3681814622432199, 'macro_accuracy': 0.5178571428571429, 'macro_precision': 0.20549450549450549, 'macro_recall': 0.20292792792792796, 'macro_f1': 0.1522017175223826, 'question_accuracy': 0.4857142857142857, 'mean_confidence': 0.9999999}
2024-12-15 03:01:46,142 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 7, 'avg_loss': 3.54823871893122, 'accuracy': 0.5142857142857142, 'precision': 0.4178990995724064, 'recall': 0.5142857142857142, 'f1': 0.4005572069709157, 'macro_accuracy': 0.5142857142857142, 'macro_precision': 0.2878537133517213, 'macro_recall': 0.22105651105651108, 'macro_f1': 0.1892312175788308, 'question_accuracy': 0.45714285714285713, 'mean_confidence': 1.0}
2024-12-15 03:04:50,019 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 8, 'avg_loss': 4.415467209384874, 'accuracy': 0.21785714285714286, 'precision': 0.34366415531729927, 'recall': 0.21785714285714286, 'f1': 0.24708704202965326, 'macro_accuracy': 0.21785714285714286, 'macro_precision': 0.2007055681903552, 'macro_recall': 0.20393939393939395, 'macro_f1': 0.18036128120776615, 'question_accuracy': 0.2571428571428571, 'mean_confidence': 0.9999663}
2024-12-15 03:07:54,075 - roberta-large_baas-prompt_commonsense_qa_binary_cluster_suffix_ratio_10 - INFO - {'epoch': 9, 'avg_loss': 4.525892566227795, 'accuracy': 0.20357142857142857, 'precision': 0.32046846913096055, 'recall': 0.20357142857142857, 'f1': 0.22465854294858578, 'macro_accuracy': 0.20357142857142857, 'macro_precision': 0.19873856097002976, 'macro_recall': 0.20797706797706797, 'macro_f1': 0.18009462756156974, 'question_accuracy': 0.22857142857142856, 'mean_confidence': 0.9902166}
