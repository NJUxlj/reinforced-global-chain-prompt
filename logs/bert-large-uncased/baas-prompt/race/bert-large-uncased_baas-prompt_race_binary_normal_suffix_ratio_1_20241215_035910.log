2024-12-15 04:04:30,016 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 0, 'avg_loss': 1.4405768067079106, 'accuracy': 0.25811688311688313, 'precision': 0.3365922136738769, 'recall': 0.25811688311688313, 'f1': 0.2802609409998268, 'macro_accuracy': 0.25811688311688313, 'macro_precision': 0.24176458970372805, 'macro_recall': 0.23920599137996862, 'macro_f1': 0.2258061334908957, 'question_accuracy': 0.262987012987013, 'mean_confidence': 0.556859}
2024-12-15 04:09:40,502 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 1, 'avg_loss': 1.4562693311243642, 'accuracy': 0.2719155844155844, 'precision': 0.35533123478198225, 'recall': 0.2719155844155844, 'f1': 0.29457247162454453, 'macro_accuracy': 0.2719155844155844, 'macro_precision': 0.2614609806133111, 'macro_recall': 0.2583003265885498, 'macro_f1': 0.24413445548715443, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.5367028}
2024-12-15 04:14:51,231 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 2, 'avg_loss': 1.4591805766344765, 'accuracy': 0.2637987012987013, 'precision': 0.34759083446613215, 'recall': 0.2637987012987013, 'f1': 0.2878644575374031, 'macro_accuracy': 0.2637987012987013, 'macro_precision': 0.2551670826015803, 'macro_recall': 0.24643316019808853, 'macro_f1': 0.23642389888425303, 'question_accuracy': 0.2824675324675325, 'mean_confidence': 0.94973445}
2024-12-15 04:19:59,608 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 3, 'avg_loss': 1.466516737340143, 'accuracy': 0.24107142857142858, 'precision': 0.3176850389890258, 'recall': 0.24107142857142858, 'f1': 0.26339114280937004, 'macro_accuracy': 0.24107142857142858, 'macro_precision': 0.23123018559065073, 'macro_recall': 0.22247993330784555, 'macro_f1': 0.21444207956564304, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.25669232}
2024-12-15 04:25:06,999 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 4, 'avg_loss': 1.4941168071229673, 'accuracy': 0.26136363636363635, 'precision': 0.33845213863592016, 'recall': 0.26136363636363635, 'f1': 0.2798061350352538, 'macro_accuracy': 0.26136363636363635, 'macro_precision': 0.2539523076304253, 'macro_recall': 0.259547286090885, 'macro_f1': 0.23898437577371723, 'question_accuracy': 0.2435064935064935, 'mean_confidence': 0.74975544}
2024-12-15 04:30:16,501 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 5, 'avg_loss': 1.5281676203446903, 'accuracy': 0.2508116883116883, 'precision': 0.32727287157750784, 'recall': 0.2508116883116883, 'f1': 0.27291919993438923, 'macro_accuracy': 0.2508116883116883, 'macro_precision': 0.233682450062896, 'macro_recall': 0.22959684212161569, 'macro_f1': 0.21795301106013026, 'question_accuracy': 0.25, 'mean_confidence': 0.9114705}
2024-12-15 04:35:26,764 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 6, 'avg_loss': 1.48830496952068, 'accuracy': 0.26055194805194803, 'precision': 0.34690694131482797, 'recall': 0.26055194805194803, 'f1': 0.2806124651088117, 'macro_accuracy': 0.26055194805194803, 'macro_precision': 0.25699808065751306, 'macro_recall': 0.25972516573641524, 'macro_f1': 0.23850625193727484, 'question_accuracy': 0.3051948051948052, 'mean_confidence': 0.8483716}
2024-12-15 04:40:34,576 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 7, 'avg_loss': 1.432124893623608, 'accuracy': 0.24756493506493507, 'precision': 0.3220832684981309, 'recall': 0.24756493506493507, 'f1': 0.26959509705611623, 'macro_accuracy': 0.24756493506493507, 'macro_precision': 0.23492478524631927, 'macro_recall': 0.22816308850316674, 'macro_f1': 0.2186261489555648, 'question_accuracy': 0.25, 'mean_confidence': 0.9720344}
2024-12-15 04:45:42,771 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 8, 'avg_loss': 1.4790464294721364, 'accuracy': 0.25487012987012986, 'precision': 0.32766349059162, 'recall': 0.25487012987012986, 'f1': 0.27556268019231456, 'macro_accuracy': 0.25487012987012986, 'macro_precision': 0.240860045653712, 'macro_recall': 0.23794104824563483, 'macro_f1': 0.22700522880850055, 'question_accuracy': 0.262987012987013, 'mean_confidence': 0.9792476}
2024-12-15 04:50:52,740 - bert-large-uncased_baas-prompt_race_binary_normal_suffix_ratio_1 - INFO - {'epoch': 9, 'avg_loss': 1.542718823380095, 'accuracy': 0.26866883116883117, 'precision': 0.33555618262118647, 'recall': 0.26866883116883117, 'f1': 0.2886837471794521, 'macro_accuracy': 0.26866883116883117, 'macro_precision': 0.2460337484010624, 'macro_recall': 0.24662799000921035, 'macro_f1': 0.2349991151751222, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.9794448}
