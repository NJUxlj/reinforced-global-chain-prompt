2024-12-15 16:32:41,286 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 0, 'avg_loss': 1.7761929135181402, 'accuracy': 0.20714285714285716, 'precision': 0.31428063901942616, 'recall': 0.20714285714285716, 'f1': 0.22059374680622104, 'macro_accuracy': 0.20714285714285716, 'macro_precision': 0.2140053582537304, 'macro_recall': 0.22815420454099233, 'macro_f1': 0.1948446565197696, 'question_accuracy': 0.18571428571428572, 'mean_confidence': 0.048282944}
2024-12-15 16:38:11,716 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 1, 'avg_loss': 1.8223767060982554, 'accuracy': 0.24285714285714285, 'precision': 0.32677892829541877, 'recall': 0.24285714285714285, 'f1': 0.2583334138096282, 'macro_accuracy': 0.24285714285714285, 'macro_precision': 0.22893127743146216, 'macro_recall': 0.24861003392319714, 'macro_f1': 0.22092744928648972, 'question_accuracy': 0.21428571428571427, 'mean_confidence': 0.951225}
2024-12-15 16:43:41,998 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 2, 'avg_loss': 2.5034950838277212, 'accuracy': 0.19642857142857142, 'precision': 0.3430341913819381, 'recall': 0.19642857142857142, 'f1': 0.2287231858374395, 'macro_accuracy': 0.19642857142857142, 'macro_precision': 0.2011761055842473, 'macro_recall': 0.1783230630898912, 'macro_f1': 0.16697232008902677, 'question_accuracy': 0.14285714285714285, 'mean_confidence': 0.5057602}
2024-12-15 16:49:11,765 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 3, 'avg_loss': 1.8387281090805405, 'accuracy': 0.225, 'precision': 0.3963432522077154, 'recall': 0.225, 'f1': 0.2566622848923563, 'macro_accuracy': 0.225, 'macro_precision': 0.23835167110322392, 'macro_recall': 0.2173586070021832, 'macro_f1': 0.19833580618976956, 'question_accuracy': 0.2, 'mean_confidence': 0.794112}
2024-12-15 16:54:42,899 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 4, 'avg_loss': 1.7742606650449728, 'accuracy': 0.14642857142857144, 'precision': 0.25757763974322984, 'recall': 0.14642857142857144, 'f1': 0.16748353289184434, 'macro_accuracy': 0.14642857142857144, 'macro_precision': 0.14985380681166457, 'macro_recall': 0.13763321367827, 'macro_f1': 0.12454804396953467, 'question_accuracy': 0.07142857142857142, 'mean_confidence': 0.13663374}
2024-12-15 17:00:09,588 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 5, 'avg_loss': 1.7961313876470453, 'accuracy': 0.20714285714285716, 'precision': 0.29831444760932596, 'recall': 0.20714285714285716, 'f1': 0.22308539271397734, 'macro_accuracy': 0.20714285714285716, 'macro_precision': 0.1996798613112408, 'macro_recall': 0.21384402903693323, 'macro_f1': 0.1866401480111008, 'question_accuracy': 0.2571428571428571, 'mean_confidence': 0.06446413}
2024-12-15 17:05:36,638 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 6, 'avg_loss': 1.9352309221499844, 'accuracy': 0.2357142857142857, 'precision': 0.3213265594024995, 'recall': 0.2357142857142857, 'f1': 0.2521214247947329, 'macro_accuracy': 0.2357142857142857, 'macro_precision': 0.22843726430946024, 'macro_recall': 0.24582337254264122, 'macro_f1': 0.22018127054800743, 'question_accuracy': 0.17142857142857143, 'mean_confidence': 0.723669}
2024-12-15 17:11:03,414 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 7, 'avg_loss': 1.822825395845269, 'accuracy': 0.18571428571428572, 'precision': 0.3037260358688931, 'recall': 0.18571428571428572, 'f1': 0.2096750441989407, 'macro_accuracy': 0.18571428571428572, 'macro_precision': 0.1857126207126207, 'macro_recall': 0.17730033620222443, 'macro_f1': 0.16089903343952897, 'question_accuracy': 0.21428571428571427, 'mean_confidence': 0.9986604}
2024-12-15 17:16:31,386 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 8, 'avg_loss': 1.8882373990981203, 'accuracy': 0.19642857142857142, 'precision': 0.3037733748226931, 'recall': 0.19642857142857142, 'f1': 0.21408306954810205, 'macro_accuracy': 0.19642857142857142, 'macro_precision': 0.196043834155913, 'macro_recall': 0.19774112448651276, 'macro_f1': 0.1764869875783302, 'question_accuracy': 0.22857142857142856, 'mean_confidence': 0.99973154}
2024-12-15 17:21:59,818 - bert-large-uncased_baas-prompt_commonsense_qa_binary_lda_suffix_ratio_10 - INFO - {'epoch': 9, 'avg_loss': 1.9255387352681474, 'accuracy': 0.2, 'precision': 0.32809349329805204, 'recall': 0.2, 'f1': 0.2185265755685732, 'macro_accuracy': 0.2, 'macro_precision': 0.20761015389386434, 'macro_recall': 0.20757952138373317, 'macro_f1': 0.18152757460260718, 'question_accuracy': 0.14285714285714285, 'mean_confidence': 0.53949356}
