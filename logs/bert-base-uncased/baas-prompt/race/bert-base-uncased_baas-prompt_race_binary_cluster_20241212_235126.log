2024-12-12 23:55:24,786 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 0, 'avg_loss': 1.410937696782215, 'accuracy': 0.26866883116883117, 'precision': 0.35321595369476966, 'recall': 0.26866883116883117, 'f1': 0.2880045764323124, 'macro_accuracy': 0.26866883116883117, 'macro_precision': 0.26614079151682046, 'macro_recall': 0.26936427523824213, 'macro_f1': 0.24900533454298512, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.74208134}
2024-12-12 23:59:09,560 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 1, 'avg_loss': 1.4007117981118293, 'accuracy': 0.26461038961038963, 'precision': 0.35717411793867465, 'recall': 0.26461038961038963, 'f1': 0.28656849676068474, 'macro_accuracy': 0.26461038961038963, 'macro_precision': 0.26097519365573224, 'macro_recall': 0.26048684943484607, 'macro_f1': 0.2402211996473464, 'question_accuracy': 0.2922077922077922, 'mean_confidence': 0.80472505}
2024-12-13 00:02:47,876 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 2, 'avg_loss': 1.3993123463917057, 'accuracy': 0.2694805194805195, 'precision': 0.3589126276154842, 'recall': 0.2694805194805195, 'f1': 0.2941412747446427, 'macro_accuracy': 0.2694805194805195, 'macro_precision': 0.2623445971264981, 'macro_recall': 0.2541143535025947, 'macro_f1': 0.24207317801815412, 'question_accuracy': 0.30194805194805197, 'mean_confidence': 0.5797867}
2024-12-13 00:06:24,979 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 3, 'avg_loss': 1.4071290475634026, 'accuracy': 0.25811688311688313, 'precision': 0.34626022926284783, 'recall': 0.25811688311688313, 'f1': 0.27952538589374215, 'macro_accuracy': 0.25811688311688313, 'macro_precision': 0.2571614851097786, 'macro_recall': 0.25366740856444336, 'macro_f1': 0.23750703312953766, 'question_accuracy': 0.23376623376623376, 'mean_confidence': 0.62242025}
2024-12-13 00:10:02,391 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 4, 'avg_loss': 1.4072010155669454, 'accuracy': 0.25811688311688313, 'precision': 0.3377110749279381, 'recall': 0.25811688311688313, 'f1': 0.2800033577782997, 'macro_accuracy': 0.25811688311688313, 'macro_precision': 0.24948177763641505, 'macro_recall': 0.2440875370853097, 'macro_f1': 0.23257256065126275, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.8459245}
2024-12-13 00:13:39,842 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 5, 'avg_loss': 1.4143990723106674, 'accuracy': 0.26055194805194803, 'precision': 0.3548896693087527, 'recall': 0.26055194805194803, 'f1': 0.28463766668083645, 'macro_accuracy': 0.26055194805194803, 'macro_precision': 0.2565101353626441, 'macro_recall': 0.2504768485909147, 'macro_f1': 0.234291155239811, 'question_accuracy': 0.2435064935064935, 'mean_confidence': 0.62679857}
2024-12-13 00:17:17,398 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 6, 'avg_loss': 1.4286643749770895, 'accuracy': 0.24756493506493507, 'precision': 0.32740474587056845, 'recall': 0.24756493506493507, 'f1': 0.2678975161470777, 'macro_accuracy': 0.24756493506493507, 'macro_precision': 0.2425867207190235, 'macro_recall': 0.24091824930963054, 'macro_f1': 0.22435254581819408, 'question_accuracy': 0.24025974025974026, 'mean_confidence': 0.93240356}
2024-12-13 00:20:54,680 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 7, 'avg_loss': 1.4354046284283564, 'accuracy': 0.2532467532467532, 'precision': 0.32872614212690277, 'recall': 0.2532467532467532, 'f1': 0.2723212668232377, 'macro_accuracy': 0.2532467532467532, 'macro_precision': 0.2456158465497017, 'macro_recall': 0.2460916509886858, 'macro_f1': 0.23069034325536256, 'question_accuracy': 0.22077922077922077, 'mean_confidence': 0.74193734}
2024-12-13 00:24:32,429 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 8, 'avg_loss': 1.427738310296751, 'accuracy': 0.2702922077922078, 'precision': 0.35568833978971914, 'recall': 0.2702922077922078, 'f1': 0.2936180212719208, 'macro_accuracy': 0.2702922077922078, 'macro_precision': 0.2625649984552698, 'macro_recall': 0.2564765542081744, 'macro_f1': 0.24398522597378153, 'question_accuracy': 0.2435064935064935, 'mean_confidence': 0.8392397}
2024-12-13 00:28:10,210 - bert-base-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 9, 'avg_loss': 1.4223899295656743, 'accuracy': 0.262987012987013, 'precision': 0.34541006130552193, 'recall': 0.262987012987013, 'f1': 0.28331971811072926, 'macro_accuracy': 0.262987012987013, 'macro_precision': 0.2584718218256473, 'macro_recall': 0.2595491252341765, 'macro_f1': 0.2405041446537297, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.9296428}
