2024-12-13 22:21:44,409 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 0, 'avg_loss': 1.4178540909950657, 'accuracy': 0.2597402597402597, 'precision': 0.34781872290312665, 'recall': 0.2597402597402597, 'f1': 0.28230151215981814, 'macro_accuracy': 0.2597402597402597, 'macro_precision': 0.2548139827151468, 'macro_recall': 0.25003129681292496, 'macro_f1': 0.2354711810373202, 'question_accuracy': 0.2922077922077922, 'mean_confidence': 0.9392506}
2024-12-13 22:25:32,837 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 1, 'avg_loss': 1.4537169769971086, 'accuracy': 0.25487012987012986, 'precision': 0.3257959335405397, 'recall': 0.25487012987012986, 'f1': 0.2725913171304756, 'macro_accuracy': 0.25487012987012986, 'macro_precision': 0.24344466881996113, 'macro_recall': 0.24958892847100572, 'macro_f1': 0.2305296818152986, 'question_accuracy': 0.22402597402597402, 'mean_confidence': 0.17193227}
2024-12-13 22:29:16,395 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 2, 'avg_loss': 1.4435328490184973, 'accuracy': 0.25162337662337664, 'precision': 0.32811769350964703, 'recall': 0.25162337662337664, 'f1': 0.2712733168086375, 'macro_accuracy': 0.25162337662337664, 'macro_precision': 0.24212024868395632, 'macro_recall': 0.24331104466797354, 'macro_f1': 0.22677682393231616, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.029032268}
2024-12-13 22:32:58,235 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 3, 'avg_loss': 1.4428046118066193, 'accuracy': 0.2637987012987013, 'precision': 0.340616733514421, 'recall': 0.2637987012987013, 'f1': 0.28608456715827363, 'macro_accuracy': 0.2637987012987013, 'macro_precision': 0.25167334559019533, 'macro_recall': 0.2462807403951725, 'macro_f1': 0.23422328552620042, 'question_accuracy': 0.25, 'mean_confidence': 0.23655418}
2024-12-13 22:36:42,499 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 4, 'avg_loss': 1.412128954170049, 'accuracy': 0.2711038961038961, 'precision': 0.3476640335385593, 'recall': 0.2711038961038961, 'f1': 0.29252459411411347, 'macro_accuracy': 0.2711038961038961, 'macro_precision': 0.25578881153357313, 'macro_recall': 0.2552239274502601, 'macro_f1': 0.24054346780638697, 'question_accuracy': 0.2727272727272727, 'mean_confidence': 0.17728637}
2024-12-13 22:40:24,741 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 5, 'avg_loss': 1.4238133656387775, 'accuracy': 0.2775974025974026, 'precision': 0.34823260738518946, 'recall': 0.2775974025974026, 'f1': 0.2951527625377438, 'macro_accuracy': 0.2775974025974026, 'macro_precision': 0.2654304633635052, 'macro_recall': 0.2737822260544729, 'macro_f1': 0.2542125189756025, 'question_accuracy': 0.2857142857142857, 'mean_confidence': 0.14836049}
2024-12-13 22:44:07,317 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 6, 'avg_loss': 1.4198783066112863, 'accuracy': 0.2508116883116883, 'precision': 0.3245113190228699, 'recall': 0.2508116883116883, 'f1': 0.26854072221473035, 'macro_accuracy': 0.2508116883116883, 'macro_precision': 0.24306805363033432, 'macro_recall': 0.24733367466315304, 'macro_f1': 0.22947083953710926, 'question_accuracy': 0.2564935064935065, 'mean_confidence': 0.39402503}
2024-12-13 22:47:51,039 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 7, 'avg_loss': 1.4199372039939502, 'accuracy': 0.26542207792207795, 'precision': 0.3502242442006385, 'recall': 0.26542207792207795, 'f1': 0.2880626215646662, 'macro_accuracy': 0.26542207792207795, 'macro_precision': 0.2531550444563504, 'macro_recall': 0.2502010837900685, 'macro_f1': 0.23539468265567598, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.40868488}
2024-12-13 22:51:34,168 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 8, 'avg_loss': 1.4179758605039503, 'accuracy': 0.2719155844155844, 'precision': 0.36058979972575156, 'recall': 0.2719155844155844, 'f1': 0.2948119381317785, 'macro_accuracy': 0.2719155844155844, 'macro_precision': 0.2643574657693177, 'macro_recall': 0.26221486001247685, 'macro_f1': 0.24500989246295415, 'question_accuracy': 0.2922077922077922, 'mean_confidence': 0.52514577}
2024-12-13 22:55:17,405 - bert-base-uncased_baas-prompt_race_binary_normal - INFO - {'epoch': 9, 'avg_loss': 1.4186904913482443, 'accuracy': 0.2483766233766234, 'precision': 0.3307777194113987, 'recall': 0.2483766233766234, 'f1': 0.26850657503421516, 'macro_accuracy': 0.2483766233766234, 'macro_precision': 0.24298325058595005, 'macro_recall': 0.2431307503987693, 'macro_f1': 0.22547066639882057, 'question_accuracy': 0.22727272727272727, 'mean_confidence': 0.57922256}
