2024-12-12 12:11:45,874 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 0, 'avg_loss': 1.4422026645652084, 'accuracy': 0.25721153846153844, 'precision': 0.33477815149738416, 'recall': 0.25721153846153844, 'f1': 0.2804545580166102, 'question_accuracy': 0.27564102564102566, 'mean_confidence': 0.29896197}
2024-12-12 12:21:13,508 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 1, 'avg_loss': 1.4470656080994495, 'accuracy': 0.25, 'precision': 0.3264732328688381, 'recall': 0.25, 'f1': 0.2713560924533863, 'question_accuracy': 0.2532051282051282, 'mean_confidence': 0.39435127}
2024-12-12 12:30:39,444 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 2, 'avg_loss': 1.4319821100595385, 'accuracy': 0.2588141025641026, 'precision': 0.3416513454155586, 'recall': 0.2588141025641026, 'f1': 0.279189529980243, 'question_accuracy': 0.266025641025641, 'mean_confidence': 0.03708481}
2024-12-12 12:40:03,551 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 3, 'avg_loss': 1.446474448019682, 'accuracy': 0.24919871794871795, 'precision': 0.318126531412924, 'recall': 0.24919871794871795, 'f1': 0.267992901159168, 'question_accuracy': 0.2467948717948718, 'mean_confidence': 0.31738478}
2024-12-12 12:49:28,819 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 4, 'avg_loss': 1.4486339275920115, 'accuracy': 0.2780448717948718, 'precision': 0.3549705732069862, 'recall': 0.2780448717948718, 'f1': 0.30029316175550835, 'question_accuracy': 0.30448717948717946, 'mean_confidence': 0.27503797}
2024-12-12 12:58:51,710 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 5, 'avg_loss': 1.4589908157670222, 'accuracy': 0.27564102564102566, 'precision': 0.362077974716108, 'recall': 0.27564102564102566, 'f1': 0.29665133829875084, 'question_accuracy': 0.28846153846153844, 'mean_confidence': 0.5274425}
2024-12-12 13:08:12,615 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 6, 'avg_loss': 1.4664555853882502, 'accuracy': 0.2588141025641026, 'precision': 0.3375294959136532, 'recall': 0.2588141025641026, 'f1': 0.2823661240862043, 'question_accuracy': 0.3108974358974359, 'mean_confidence': 0.6352202}
2024-12-12 13:17:33,854 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 7, 'avg_loss': 1.4963371774484946, 'accuracy': 0.2668269230769231, 'precision': 0.33156216672645755, 'recall': 0.2668269230769231, 'f1': 0.284919381288168, 'question_accuracy': 0.2916666666666667, 'mean_confidence': 8.3864e-05}
2024-12-12 13:26:56,439 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 8, 'avg_loss': 1.5559348919710447, 'accuracy': 0.24759615384615385, 'precision': 0.3345122084829125, 'recall': 0.24759615384615385, 'f1': 0.2666023505832379, 'question_accuracy': 0.23717948717948717, 'mean_confidence': 0.029355869}
2024-12-12 13:36:18,368 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 9, 'avg_loss': 1.5711559119612672, 'accuracy': 0.27003205128205127, 'precision': 0.3424453335516856, 'recall': 0.27003205128205127, 'f1': 0.2917019531042358, 'question_accuracy': 0.266025641025641, 'mean_confidence': 0.3059997}
2024-12-12 13:45:38,116 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 10, 'avg_loss': 1.5709905972660974, 'accuracy': 0.27163461538461536, 'precision': 0.35222310266629364, 'recall': 0.27163461538461536, 'f1': 0.2926532605415131, 'question_accuracy': 0.266025641025641, 'mean_confidence': 0.5460558}
2024-12-12 13:54:57,755 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 11, 'avg_loss': 1.5860983616737432, 'accuracy': 0.24599358974358973, 'precision': 0.32836508270774617, 'recall': 0.24599358974358973, 'f1': 0.2720665675183848, 'question_accuracy': 0.24358974358974358, 'mean_confidence': 0.88883543}
2024-12-12 14:04:19,048 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 12, 'avg_loss': 1.55147470741771, 'accuracy': 0.2692307692307692, 'precision': 0.34536420903809745, 'recall': 0.2692307692307692, 'f1': 0.2922228153339907, 'question_accuracy': 0.24358974358974358, 'mean_confidence': 0.95660675}
2024-12-12 14:13:39,725 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 13, 'avg_loss': 1.5166654099905215, 'accuracy': 0.265224358974359, 'precision': 0.3429616941167537, 'recall': 0.265224358974359, 'f1': 0.28456393471678804, 'question_accuracy': 0.27884615384615385, 'mean_confidence': 0.7483639}
2024-12-12 14:23:00,379 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 14, 'avg_loss': 1.4926231531209724, 'accuracy': 0.28846153846153844, 'precision': 0.3627251968126777, 'recall': 0.28846153846153844, 'f1': 0.3063024412700574, 'question_accuracy': 0.30128205128205127, 'mean_confidence': 0.6871647}
2024-12-12 14:32:21,147 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 15, 'avg_loss': 1.4811853087225626, 'accuracy': 0.25961538461538464, 'precision': 0.3316434335958954, 'recall': 0.25961538461538464, 'f1': 0.27876791711048976, 'question_accuracy': 0.28205128205128205, 'mean_confidence': 0.8032939}
2024-12-12 14:41:41,741 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 16, 'avg_loss': 1.4673244710578475, 'accuracy': 0.27884615384615385, 'precision': 0.3596666368491094, 'recall': 0.27884615384615385, 'f1': 0.3001514646376862, 'question_accuracy': 0.3141025641025641, 'mean_confidence': 0.5079303}
2024-12-12 14:51:04,023 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 17, 'avg_loss': 1.4468439238709072, 'accuracy': 0.2748397435897436, 'precision': 0.35700855959671596, 'recall': 0.2748397435897436, 'f1': 0.2942948182106495, 'question_accuracy': 0.28205128205128205, 'mean_confidence': 0.27245867}
2024-12-12 15:00:25,096 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 18, 'avg_loss': 1.4495357181443724, 'accuracy': 0.2876602564102564, 'precision': 0.37194453058751525, 'recall': 0.2876602564102564, 'f1': 0.30849540661123764, 'question_accuracy': 0.2980769230769231, 'mean_confidence': 0.5412722}
2024-12-12 15:09:47,280 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 19, 'avg_loss': 1.4284869200268457, 'accuracy': 0.27884615384615385, 'precision': 0.3617575677341752, 'recall': 0.27884615384615385, 'f1': 0.29972095883844535, 'question_accuracy': 0.27564102564102566, 'mean_confidence': 0.5399951}
