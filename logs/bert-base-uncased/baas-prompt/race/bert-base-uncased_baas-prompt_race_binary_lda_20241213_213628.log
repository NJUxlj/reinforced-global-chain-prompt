2024-12-13 21:40:22,058 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 0, 'avg_loss': 1.4139498465262765, 'accuracy': 0.26542207792207795, 'precision': 0.35350256230470417, 'recall': 0.26542207792207795, 'f1': 0.28585712330937413, 'macro_accuracy': 0.26542207792207795, 'macro_precision': 0.2603282474179816, 'macro_recall': 0.2620597253973812, 'macro_f1': 0.2426607862630626, 'question_accuracy': 0.2824675324675325, 'mean_confidence': 0.66903174}
2024-12-13 21:44:00,655 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 1, 'avg_loss': 1.3969524528820383, 'accuracy': 0.286525974025974, 'precision': 0.37978380736979367, 'recall': 0.286525974025974, 'f1': 0.30388560303559675, 'macro_accuracy': 0.286525974025974, 'macro_precision': 0.290019639907943, 'macro_recall': 0.3014358098349251, 'macro_f1': 0.27107041778099616, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.86752516}
2024-12-13 21:47:34,020 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 2, 'avg_loss': 1.394706662472761, 'accuracy': 0.2767857142857143, 'precision': 0.365000999598866, 'recall': 0.2767857142857143, 'f1': 0.29369759420494496, 'macro_accuracy': 0.2767857142857143, 'macro_precision': 0.27692086089990875, 'macro_recall': 0.288808681366132, 'macro_f1': 0.2597277596818284, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.9100195}
2024-12-13 21:51:05,603 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 3, 'avg_loss': 1.39160074228448, 'accuracy': 0.26785714285714285, 'precision': 0.35766014978016397, 'recall': 0.26785714285714285, 'f1': 0.28711709645189026, 'macro_accuracy': 0.26785714285714285, 'macro_precision': 0.2675620242593285, 'macro_recall': 0.2715700560501255, 'macro_f1': 0.2487578559786968, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.91293293}
2024-12-13 21:54:37,129 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 4, 'avg_loss': 1.3984309890179871, 'accuracy': 0.275974025974026, 'precision': 0.36362640375058636, 'recall': 0.275974025974026, 'f1': 0.2935465314125451, 'macro_accuracy': 0.275974025974026, 'macro_precision': 0.2753889670233313, 'macro_recall': 0.28137141953467376, 'macro_f1': 0.2584323294124546, 'question_accuracy': 0.2727272727272727, 'mean_confidence': 0.93113273}
2024-12-13 21:58:09,311 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 5, 'avg_loss': 1.4026482880289284, 'accuracy': 0.2767857142857143, 'precision': 0.3655854705460619, 'recall': 0.2767857142857143, 'f1': 0.2963812673718136, 'macro_accuracy': 0.2767857142857143, 'macro_precision': 0.27378132965165675, 'macro_recall': 0.2763310023401594, 'macro_f1': 0.2563094892431693, 'question_accuracy': 0.288961038961039, 'mean_confidence': 0.94554245}
2024-12-13 22:01:41,450 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 6, 'avg_loss': 1.4081853103359656, 'accuracy': 0.28084415584415584, 'precision': 0.36736650432220813, 'recall': 0.28084415584415584, 'f1': 0.29835861315018175, 'macro_accuracy': 0.28084415584415584, 'macro_precision': 0.2792469588398806, 'macro_recall': 0.2897766644847063, 'macro_f1': 0.2626909889044307, 'question_accuracy': 0.24025974025974026, 'mean_confidence': 0.9670632}
2024-12-13 22:05:13,097 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 7, 'avg_loss': 1.4016351312312023, 'accuracy': 0.2767857142857143, 'precision': 0.37571217287978437, 'recall': 0.2767857142857143, 'f1': 0.2937702777140338, 'macro_accuracy': 0.2767857142857143, 'macro_precision': 0.2835323108532567, 'macro_recall': 0.2925324422361014, 'macro_f1': 0.26214588012769935, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.9641882}
2024-12-13 22:08:44,306 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 8, 'avg_loss': 1.4046811056554143, 'accuracy': 0.2857142857142857, 'precision': 0.394123689581162, 'recall': 0.2857142857142857, 'f1': 0.305179078468582, 'macro_accuracy': 0.2857142857142857, 'macro_precision': 0.2925164484870613, 'macro_recall': 0.2987533673648322, 'macro_f1': 0.2686562969483932, 'question_accuracy': 0.2824675324675325, 'mean_confidence': 0.97448176}
2024-12-13 22:12:15,579 - bert-base-uncased_baas-prompt_race_binary_lda - INFO - {'epoch': 9, 'avg_loss': 1.4417786347970323, 'accuracy': 0.27516233766233766, 'precision': 0.37257098537319566, 'recall': 0.27516233766233766, 'f1': 0.29478450608126966, 'macro_accuracy': 0.27516233766233766, 'macro_precision': 0.28053701063002334, 'macro_recall': 0.2815965218523365, 'macro_f1': 0.25907351899771036, 'question_accuracy': 0.2824675324675325, 'mean_confidence': 0.9702763}
