2024-12-15 14:19:34,846 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 0, 'avg_loss': 1.4257757151092108, 'accuracy': 0.25162337662337664, 'precision': 0.3360296036308882, 'recall': 0.25162337662337664, 'f1': 0.27213772984121826, 'macro_accuracy': 0.25162337662337664, 'macro_precision': 0.2449725319969885, 'macro_recall': 0.24425546216617716, 'macro_f1': 0.2280451823693103, 'question_accuracy': 0.2792207792207792, 'mean_confidence': 0.768063}
2024-12-15 14:24:52,859 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 1, 'avg_loss': 1.4438402835203676, 'accuracy': 0.2694805194805195, 'precision': 0.3520020026655725, 'recall': 0.2694805194805195, 'f1': 0.29185178520546884, 'macro_accuracy': 0.2694805194805195, 'macro_precision': 0.2580061062729392, 'macro_recall': 0.2551613435247603, 'macro_f1': 0.24152397880386062, 'question_accuracy': 0.2792207792207792, 'mean_confidence': 0.86623186}
2024-12-15 14:30:13,057 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 2, 'avg_loss': 1.4810188556899135, 'accuracy': 0.27435064935064934, 'precision': 0.3632975036904582, 'recall': 0.27435064935064934, 'f1': 0.29566297158147353, 'macro_accuracy': 0.27435064935064934, 'macro_precision': 0.2675838878005994, 'macro_recall': 0.26995827033796066, 'macro_f1': 0.24979457208844708, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.6281215}
2024-12-15 14:35:28,709 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 3, 'avg_loss': 1.597676531045152, 'accuracy': 0.5073051948051948, 'precision': 0.35112073896739476, 'recall': 0.5073051948051948, 'f1': 0.3453300140695099, 'macro_accuracy': 0.5073051948051948, 'macro_precision': 0.2518352365415987, 'macro_recall': 0.2524314765694076, 'macro_f1': 0.17441062911651148, 'question_accuracy': 0.5454545454545454, 'mean_confidence': 1.0}
2024-12-15 14:40:50,367 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 4, 'avg_loss': 1.6428804392369774, 'accuracy': 0.40665584415584416, 'precision': 0.3529758346229815, 'recall': 0.4066558441558441, 'f1': 0.3706588421859357, 'macro_accuracy': 0.40665584415584416, 'macro_precision': 0.258606728905448, 'macro_recall': 0.2602733719415319, 'macro_f1': 0.25009880850709676, 'question_accuracy': 0.4383116883116883, 'mean_confidence': 1.0}
2024-12-15 14:46:14,171 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 5, 'avg_loss': 1.7594597605505073, 'accuracy': 0.2532467532467532, 'precision': 0.34352768449206333, 'recall': 0.2532467532467532, 'f1': 0.2772083827400543, 'macro_accuracy': 0.2532467532467532, 'macro_precision': 0.2471262332981722, 'macro_recall': 0.23789639525600742, 'macro_f1': 0.22702845912539205, 'question_accuracy': 0.2435064935064935, 'mean_confidence': 0.99996513}
2024-12-15 14:51:37,517 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 6, 'avg_loss': 1.7355034530336586, 'accuracy': 0.25811688311688313, 'precision': 0.3402243816615191, 'recall': 0.25811688311688313, 'f1': 0.2802142227451287, 'macro_accuracy': 0.25811688311688313, 'macro_precision': 0.248076371045819, 'macro_recall': 0.24298928150519047, 'macro_f1': 0.23143691984017215, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.99991816}
2024-12-15 14:57:01,893 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 7, 'avg_loss': 2.0738550113519496, 'accuracy': 0.2418831168831169, 'precision': 0.32815498835867646, 'recall': 0.2418831168831169, 'f1': 0.26299622701655484, 'macro_accuracy': 0.2418831168831169, 'macro_precision': 0.23685977427450758, 'macro_recall': 0.23339281689502983, 'macro_f1': 0.21851191586115362, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.8265246}
2024-12-15 15:02:24,532 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 8, 'avg_loss': 1.887113172267686, 'accuracy': 0.2597402597402597, 'precision': 0.339590719105698, 'recall': 0.2597402597402597, 'f1': 0.27991585011077913, 'macro_accuracy': 0.2597402597402597, 'macro_precision': 0.2498918804440277, 'macro_recall': 0.25088996926721047, 'macro_f1': 0.23468194031669826, 'question_accuracy': 0.262987012987013, 'mean_confidence': 0.99905616}
2024-12-15 15:07:46,413 - bert-large-uncased_baas-prompt_race_binary_lda_suffix_ratio_1 - INFO - {'epoch': 9, 'avg_loss': 1.9552998813004951, 'accuracy': 0.2702922077922078, 'precision': 0.34654426131924115, 'recall': 0.2702922077922078, 'f1': 0.288706952667332, 'macro_accuracy': 0.2702922077922078, 'macro_precision': 0.2615158831081649, 'macro_recall': 0.26691061246247266, 'macro_f1': 0.2481847883866616, 'question_accuracy': 0.3051948051948052, 'mean_confidence': 0.9999086}
