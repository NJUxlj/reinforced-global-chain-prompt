2024-12-15 20:52:56,368 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 0, 'avg_loss': 1.416328243020672, 'accuracy': 0.2792207792207792, 'precision': 0.36214233500468784, 'recall': 0.2792207792207792, 'f1': 0.2996811039095423, 'macro_accuracy': 0.2792207792207792, 'macro_precision': 0.26909982410647393, 'macro_recall': 0.27263400043919034, 'macro_f1': 0.2540252779227108, 'question_accuracy': 0.288961038961039, 'mean_confidence': 0.9060847}
2024-12-15 20:55:48,677 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 1, 'avg_loss': 1.4983783491151326, 'accuracy': 0.2597402597402597, 'precision': 0.3430383642123462, 'recall': 0.2597402597402597, 'f1': 0.2808402887154151, 'macro_accuracy': 0.2597402597402597, 'macro_precision': 0.2530484554070075, 'macro_recall': 0.2516465983288651, 'macro_f1': 0.23598647170053205, 'question_accuracy': 0.24675324675324675, 'mean_confidence': 0.2594163}
2024-12-15 20:58:38,310 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 2, 'avg_loss': 1.4730976667070528, 'accuracy': 0.26785714285714285, 'precision': 0.3670275004018169, 'recall': 0.26785714285714285, 'f1': 0.29334938004732314, 'macro_accuracy': 0.26785714285714285, 'macro_precision': 0.2645388251675498, 'macro_recall': 0.25510732022029026, 'macro_f1': 0.24179072670999815, 'question_accuracy': 0.275974025974026, 'mean_confidence': 0.249882}
2024-12-15 21:01:29,589 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 3, 'avg_loss': 1.5447417398583436, 'accuracy': 0.25487012987012986, 'precision': 0.34500482397105775, 'recall': 0.25487012987012986, 'f1': 0.27822771303215477, 'macro_accuracy': 0.25487012987012986, 'macro_precision': 0.249496859245117, 'macro_recall': 0.24317370753506282, 'macro_f1': 0.22931534767543824, 'question_accuracy': 0.2305194805194805, 'mean_confidence': 0.26659945}
2024-12-15 21:04:18,125 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 4, 'avg_loss': 1.5907984714813899, 'accuracy': 0.2662337662337662, 'precision': 0.3570017781096751, 'recall': 0.2662337662337662, 'f1': 0.2888084914085267, 'macro_accuracy': 0.2662337662337662, 'macro_precision': 0.26067359742569596, 'macro_recall': 0.25803488205844394, 'macro_f1': 0.2410043383339869, 'question_accuracy': 0.22402597402597402, 'mean_confidence': 0.15238616}
2024-12-15 21:07:09,572 - bert-large-uncased_baas-prompt_race_binary_cluster_suffix_ratio_1 - INFO - {'epoch': 5, 'avg_loss': 1.6097850084478584, 'accuracy': 0.26055194805194803, 'precision': 0.3545529746737472, 'recall': 0.26055194805194803, 'f1': 0.2846418395776651, 'macro_accuracy': 0.26055194805194803, 'macro_precision': 0.25673595974398905, 'macro_recall': 0.2499835715045533, 'macro_f1': 0.23519765539364146, 'question_accuracy': 0.2564935064935065, 'mean_confidence': 0.052842375}
