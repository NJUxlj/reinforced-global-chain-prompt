2024-12-15 16:02:19,796 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 0, 'avg_loss': 1.121613689430097, 'accuracy': 0.3968253968253968, 'precision': 0.5177472872388126, 'recall': 0.3968253968253968, 'f1': 0.4228998865923403, 'macro_accuracy': 0.3968253968253968, 'macro_precision': 0.39156308851224103, 'macro_recall': 0.409277008638031, 'macro_f1': 0.3691090747569153, 'question_accuracy': 0.3888888888888889, 'mean_confidence': 0.9066991}
2024-12-15 16:04:49,193 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 1, 'avg_loss': 1.127774804674518, 'accuracy': 0.3333333333333333, 'precision': 0.46095523433074137, 'recall': 0.3333333333333333, 'f1': 0.3621748523433917, 'macro_accuracy': 0.3333333333333333, 'macro_precision': 0.3342629493885587, 'macro_recall': 0.3352778763854376, 'macro_f1': 0.30485022619854085, 'question_accuracy': 0.3412698412698413, 'mean_confidence': 0.6941749}
2024-12-15 16:07:21,342 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 2, 'avg_loss': 1.2798563351571872, 'accuracy': 0.3273809523809524, 'precision': 0.45452838422526304, 'recall': 0.3273809523809524, 'f1': 0.35744796081062014, 'macro_accuracy': 0.3273809523809524, 'macro_precision': 0.32787054215625644, 'macro_recall': 0.325102043987383, 'macro_f1': 0.29748188511006124, 'question_accuracy': 0.2619047619047619, 'mean_confidence': 0.4182782}
2024-12-15 16:09:51,750 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 3, 'avg_loss': 1.2373878994849339, 'accuracy': 0.3412698412698413, 'precision': 0.4646317123979891, 'recall': 0.3412698412698413, 'f1': 0.3740418892005388, 'macro_accuracy': 0.3412698412698413, 'macro_precision': 0.3328596104184837, 'macro_recall': 0.32537133254208156, 'macro_f1': 0.3044156152675423, 'question_accuracy': 0.29365079365079366, 'mean_confidence': 0.64082336}
2024-12-15 16:12:23,885 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 4, 'avg_loss': 1.1602021305972994, 'accuracy': 0.36706349206349204, 'precision': 0.4939478941083164, 'recall': 0.36706349206349204, 'f1': 0.4025723251006843, 'macro_accuracy': 0.36706349206349204, 'macro_precision': 0.35139279139099105, 'macro_recall': 0.34226611159518505, 'macro_f1': 0.32286294579793107, 'question_accuracy': 0.30158730158730157, 'mean_confidence': 0.466525}
2024-12-15 16:14:55,735 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 5, 'avg_loss': 1.1538505780447217, 'accuracy': 0.3392857142857143, 'precision': 0.46569170409698185, 'recall': 0.3392857142857143, 'f1': 0.37225629589430437, 'macro_accuracy': 0.3392857142857143, 'macro_precision': 0.3315915909056299, 'macro_recall': 0.32322144650863144, 'macro_f1': 0.3024602874995417, 'question_accuracy': 0.373015873015873, 'mean_confidence': 0.46575418}
2024-12-15 16:17:26,671 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 6, 'avg_loss': 1.2588611663324047, 'accuracy': 0.3253968253968254, 'precision': 0.44572850676799336, 'recall': 0.3253968253968254, 'f1': 0.3527192583076427, 'macro_accuracy': 0.3253968253968254, 'macro_precision': 0.3247097833439881, 'macro_recall': 0.3281722681793679, 'macro_f1': 0.29713240791108914, 'question_accuracy': 0.3333333333333333, 'mean_confidence': 0.3610553}
2024-12-15 16:19:58,296 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 7, 'avg_loss': 1.3349453628999401, 'accuracy': 0.32142857142857145, 'precision': 0.4394820078050108, 'recall': 0.32142857142857145, 'f1': 0.35022376097288055, 'macro_accuracy': 0.32142857142857145, 'macro_precision': 0.31721237727227003, 'macro_recall': 0.31343227566159765, 'macro_f1': 0.29057658263109737, 'question_accuracy': 0.2857142857142857, 'mean_confidence': 0.43639147}
2024-12-15 16:22:29,828 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 8, 'avg_loss': 1.4144943435310693, 'accuracy': 0.34523809523809523, 'precision': 0.45441407142637413, 'recall': 0.34523809523809523, 'f1': 0.37635137426786164, 'macro_accuracy': 0.34523809523809523, 'macro_precision': 0.3260324381690661, 'macro_recall': 0.32160523708269534, 'macro_f1': 0.30294024185027196, 'question_accuracy': 0.35714285714285715, 'mean_confidence': 0.5006062}
2024-12-15 16:25:01,620 - bert-large-uncased_baas-prompt_dream_binary_lda_suffix_ratio_5 - INFO - {'epoch': 9, 'avg_loss': 1.5040486703836482, 'accuracy': 0.3392857142857143, 'precision': 0.4420784263526199, 'recall': 0.3392857142857143, 'f1': 0.36534908011294515, 'macro_accuracy': 0.3392857142857143, 'macro_precision': 0.32602150537634406, 'macro_recall': 0.3323722373420634, 'macro_f1': 0.30654250525559407, 'question_accuracy': 0.36507936507936506, 'mean_confidence': 0.3020174}
