2024-12-13 01:36:38,052 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 0, 'avg_loss': 1.466360754591383, 'accuracy': 0.2418831168831169, 'precision': 0.3283772664007748, 'recall': 0.2418831168831169, 'f1': 0.26268670417694256, 'macro_accuracy': 0.2418831168831169, 'macro_precision': 0.24326726815455021, 'macro_recall': 0.23717026344907902, 'macro_f1': 0.2233912845791649, 'question_accuracy': 0.21103896103896103, 'mean_confidence': 0.2762519}
2024-12-13 01:46:01,899 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 1, 'avg_loss': 1.454075355571491, 'accuracy': 0.262987012987013, 'precision': 0.35513400037626275, 'recall': 0.262987012987013, 'f1': 0.28996264603793614, 'macro_accuracy': 0.262987012987013, 'macro_precision': 0.2522523607413535, 'macro_recall': 0.2382359956322699, 'macro_f1': 0.23076043032494428, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.006147904}
2024-12-13 01:55:28,044 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 2, 'avg_loss': 1.4831092269010515, 'accuracy': 0.26704545454545453, 'precision': 0.362509959501696, 'recall': 0.26704545454545453, 'f1': 0.289376892905566, 'macro_accuracy': 0.26704545454545453, 'macro_precision': 0.2645180691598774, 'macro_recall': 0.26346653257658215, 'macro_f1': 0.24334324540860347, 'question_accuracy': 0.2532467532467532, 'mean_confidence': 0.0030276927}
2024-12-13 02:04:53,311 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 3, 'avg_loss': 1.4875198307259785, 'accuracy': 0.273538961038961, 'precision': 0.35676806135498973, 'recall': 0.273538961038961, 'f1': 0.2926049770163905, 'macro_accuracy': 0.273538961038961, 'macro_precision': 0.2701807998159009, 'macro_recall': 0.2746990044920171, 'macro_f1': 0.2538318968809009, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.0039051538}
2024-12-13 02:14:18,778 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 4, 'avg_loss': 1.5630281511965716, 'accuracy': 0.273538961038961, 'precision': 0.35649826018857633, 'recall': 0.273538961038961, 'f1': 0.2938537331343116, 'macro_accuracy': 0.273538961038961, 'macro_precision': 0.2647590485600887, 'macro_recall': 0.26790714875082416, 'macro_f1': 0.2486667769143226, 'question_accuracy': 0.22077922077922077, 'mean_confidence': 0.0006469375}
2024-12-13 02:23:42,228 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 5, 'avg_loss': 1.587942955792819, 'accuracy': 0.25243506493506496, 'precision': 0.3340810668957118, 'recall': 0.25243506493506496, 'f1': 0.2727462563118803, 'macro_accuracy': 0.25243506493506496, 'macro_precision': 0.2461808667802997, 'macro_recall': 0.24525406226716884, 'macro_f1': 0.22948622363599305, 'question_accuracy': 0.2564935064935065, 'mean_confidence': 0.011642685}
2024-12-13 02:33:06,787 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 6, 'avg_loss': 1.6767728064567633, 'accuracy': 0.2694805194805195, 'precision': 0.35367662203390093, 'recall': 0.2694805194805195, 'f1': 0.28603749822735547, 'macro_accuracy': 0.2694805194805195, 'macro_precision': 0.2694302204781247, 'macro_recall': 0.2784725694427471, 'macro_f1': 0.25344202994508785, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.9995822}
2024-12-13 02:42:31,633 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 7, 'avg_loss': 1.6984016109833564, 'accuracy': 0.26055194805194803, 'precision': 0.356238061486887, 'recall': 0.26055194805194803, 'f1': 0.28476326651586414, 'macro_accuracy': 0.26055194805194803, 'macro_precision': 0.2564439768192899, 'macro_recall': 0.24986023199728863, 'macro_f1': 0.23422341991612225, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.99301076}
2024-12-13 02:51:56,284 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 8, 'avg_loss': 1.744403501815073, 'accuracy': 0.2508116883116883, 'precision': 0.3251908841184156, 'recall': 0.2508116883116883, 'f1': 0.2732316880095139, 'macro_accuracy': 0.2508116883116883, 'macro_precision': 0.23312094882168252, 'macro_recall': 0.2265675324772046, 'macro_f1': 0.21779929142258614, 'question_accuracy': 0.2792207792207792, 'mean_confidence': 1.934124e-06}
2024-12-13 03:01:21,309 - bert-large-uncased_baas-prompt_race_binary_cluster - INFO - {'epoch': 9, 'avg_loss': 1.8658453535408042, 'accuracy': 0.26136363636363635, 'precision': 0.342498197424253, 'recall': 0.26136363636363635, 'f1': 0.2823762936601583, 'macro_accuracy': 0.26136363636363635, 'macro_precision': 0.24797651992109057, 'macro_recall': 0.24916432989624912, 'macro_f1': 0.2316961320813802, 'question_accuracy': 0.2597402597402597, 'mean_confidence': 0.00017268375}
