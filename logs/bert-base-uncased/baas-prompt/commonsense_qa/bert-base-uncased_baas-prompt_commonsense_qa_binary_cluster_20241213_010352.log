2024-12-13 01:05:52,591 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 0, 'avg_loss': 1.6204700250374644, 'accuracy': 0.16911764705882354, 'precision': 0.2642436958723823, 'recall': 0.16911764705882354, 'f1': 0.19449749205840075, 'macro_accuracy': 0.16911764705882354, 'macro_precision': 0.15939276940246624, 'macro_recall': 0.15302080985650962, 'macro_f1': 0.13981972713315996, 'question_accuracy': 0.10294117647058823, 'mean_confidence': 0.377763}
2024-12-13 01:07:42,864 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 1, 'avg_loss': 1.6173493462173563, 'accuracy': 0.17279411764705882, 'precision': 0.27425228972666615, 'recall': 0.17279411764705882, 'f1': 0.19755616832180634, 'macro_accuracy': 0.17279411764705882, 'macro_precision': 0.16595244761045613, 'macro_recall': 0.16366463826910074, 'macro_f1': 0.14564530751365262, 'question_accuracy': 0.1323529411764706, 'mean_confidence': 0.5933104}
2024-12-13 01:09:31,951 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 2, 'avg_loss': 1.6307233666118823, 'accuracy': 0.22794117647058823, 'precision': 0.33906341817674335, 'recall': 0.22794117647058823, 'f1': 0.2557841170326311, 'macro_accuracy': 0.22794117647058823, 'macro_precision': 0.21426308236105968, 'macro_recall': 0.22090601757944556, 'macro_f1': 0.1931406447885493, 'question_accuracy': 0.27941176470588236, 'mean_confidence': 0.3169285}
2024-12-13 01:11:21,703 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 3, 'avg_loss': 1.631991384845031, 'accuracy': 0.18382352941176472, 'precision': 0.27405028823695066, 'recall': 0.18382352941176472, 'f1': 0.20503729957329467, 'macro_accuracy': 0.18382352941176472, 'macro_precision': 0.1689930848900153, 'macro_recall': 0.17149575539027873, 'macro_f1': 0.15233672346826518, 'question_accuracy': 0.16176470588235295, 'mean_confidence': 0.36443734}
2024-12-13 01:13:11,188 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 4, 'avg_loss': 1.6488337736380727, 'accuracy': 0.20955882352941177, 'precision': 0.30322929597975556, 'recall': 0.20955882352941177, 'f1': 0.23130434776862183, 'macro_accuracy': 0.20955882352941177, 'macro_precision': 0.19616612554112553, 'macro_recall': 0.20242506197881452, 'macro_f1': 0.18176330489668827, 'question_accuracy': 0.3088235294117647, 'mean_confidence': 0.30854356}
2024-12-13 01:15:00,679 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 5, 'avg_loss': 1.620118844665979, 'accuracy': 0.1948529411764706, 'precision': 0.2795775183687666, 'recall': 0.1948529411764706, 'f1': 0.21587972401204347, 'macro_accuracy': 0.1948529411764706, 'macro_precision': 0.17709036805163061, 'macro_recall': 0.17552851025467658, 'macro_f1': 0.16321126908568565, 'question_accuracy': 0.22058823529411764, 'mean_confidence': 0.76208264}
2024-12-13 01:16:50,166 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 6, 'avg_loss': 1.6077108038099188, 'accuracy': 0.23529411764705882, 'precision': 0.34552252921324683, 'recall': 0.23529411764705882, 'f1': 0.2619854389049788, 'macro_accuracy': 0.23529411764705882, 'macro_precision': 0.21489126381316317, 'macro_recall': 0.210230636315829, 'macro_f1': 0.19289866925384255, 'question_accuracy': 0.29411764705882354, 'mean_confidence': 0.7741875}
2024-12-13 01:18:39,480 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 7, 'avg_loss': 1.6274018695479946, 'accuracy': 0.20220588235294118, 'precision': 0.2804362660253563, 'recall': 0.20220588235294118, 'f1': 0.22032176159832778, 'macro_accuracy': 0.20220588235294118, 'macro_precision': 0.18858633614447567, 'macro_recall': 0.19961084817068592, 'macro_f1': 0.1780215525425653, 'question_accuracy': 0.25, 'mean_confidence': 0.7051518}
2024-12-13 01:20:28,564 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 8, 'avg_loss': 1.6325521241677434, 'accuracy': 0.20220588235294118, 'precision': 0.2866025651455493, 'recall': 0.20220588235294118, 'f1': 0.22157863742093098, 'macro_accuracy': 0.20220588235294118, 'macro_precision': 0.17310221894400393, 'macro_recall': 0.17751333483585002, 'macro_f1': 0.15939447223908507, 'question_accuracy': 0.25, 'mean_confidence': 0.8694029}
2024-12-13 01:22:18,202 - bert-base-uncased_baas-prompt_commonsense_qa_binary_cluster - INFO - {'epoch': 9, 'avg_loss': 1.630142195444358, 'accuracy': 0.21323529411764705, 'precision': 0.310245989436484, 'recall': 0.21323529411764705, 'f1': 0.23552921384448927, 'macro_accuracy': 0.21323529411764705, 'macro_precision': 0.19721498120894243, 'macro_recall': 0.20172789422282325, 'macro_f1': 0.18152592404965637, 'question_accuracy': 0.23529411764705882, 'mean_confidence': 0.79988587}
